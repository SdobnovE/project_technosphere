{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from html_sanitizer import Sanitizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random as rnd\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import bs4\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"TitlesTrain.csv\")\n",
    "data_1 = pd.read_csv(\"mean_text.csv\")\n",
    "y_train = np.array(data[\"target\"])\n",
    "groups_train = np.array(data[\"group\"])\n",
    "texts = np.array(data[\"information\"])\n",
    "texts_1 = np.array(data_1[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.merge(data, data_1, left_on='doc_id', right_on='doc_id',how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = []\n",
    "# for i in range(data.shape[0]):\n",
    "#     #print(data.iloc[i][\"information\"],data.iloc[i][\"word\"])\n",
    "#     texts.append(str(data.iloc[i][\"information\"]) + \" \" + str(data.iloc[i][\"word\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.concatenate((texts,texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    if texts[i] is np.nan:\n",
    "        texts[i] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X_train = vect.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4c6d7072eb79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mneigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mXTrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroups_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mX_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroups_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "neigh = 25\n",
    "XTrain = []\n",
    "for i in np.unique(groups_train):\n",
    "    X_ = X_train[groups_train == i]\n",
    "    \n",
    "    distMart = sklearn.metrics.pairwise_distances(X_, metric=\"cosine\")\n",
    "    #print(distMart)\n",
    "    meanDist = np.sort(distMart, axis=1)[:,4:neigh]\n",
    "    lis = []\n",
    "    meanD = np.sort(distMart, axis=1)[:,:].mean(axis=1)\n",
    "    stdD = np.sort(distMart, axis=1)[:,:].std(axis=1)\n",
    "    \n",
    "    \n",
    "    DIST = sklearn.metrics.pairwise.cosine_distances(X_,  X_train[-129 - 1 + i, :])\n",
    "    #print(distMart)\n",
    "    #print(meanD.shape)\n",
    "    for i in range(meanDist.shape[0]):\n",
    "        rr = [j for j in meanDist[i,:]]\n",
    "        rr.append(meanD[i])\n",
    "        rr.append(stdD[i])\n",
    "        #rr.append(DIST[i])\n",
    "        XTrain.append(rr)\n",
    "        #print(i.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#XTrain = np.array(XTrain)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrain = np.array(XTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 23)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DBSCAN_VECTOR.txt\", \"r\") as f:\n",
    "    lis = [int(i) for i in f.read().split()]\n",
    "    #print(lis)\n",
    "    XTrain = np.append(XTrain, np.array(lis).reshape((-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"KMeans_matr.txt\", \"r\") as f:\n",
    "#     lis = [int(i) for i in f.read().split()]\n",
    "#     lis = np.array(lis).reshape((-1,23))  \n",
    "#     print(lis.shape, XTrain.shape)\n",
    "#     XTrain = np.append(XTrain, lis, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scl = StandardScaler()\n",
    "# scl.fit(XTrain)\n",
    "# XTrain = scl.transform(XTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11690, 24), (11690,))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 24)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_big_matr(matr):\n",
    "    matr1 = matr[:,:]\n",
    "    for i in range(matr.shape[1]):\n",
    "        for j in range(i, matr.shape[1]):\n",
    "                #print((matr[:,i] * matr[:,j]).shape, matr1.shape)\n",
    "                matr1 = np.concatenate((matr1, matr[:,i].reshape((-1,1)) * matr[:,j].reshape((-1,1))), axis=1)\n",
    "    return matr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XTrain = make_big_matr(XTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\there\n",
      "0.969341251584702\n",
      "0.8422941213470196\n",
      "(array([0, 1]), array([884, 568]))\n",
      "F1  0.8007213706041479\n",
      "\there\n",
      "0.9676431843669586\n",
      "0.8271868738699115\n",
      "(array([0, 1]), array([1023,  434]))\n",
      "F1  0.7297297297297296\n",
      "\there\n",
      "0.9697054888360469\n",
      "0.7849795678459472\n",
      "(array([0, 1]), array([1052,  410]))\n",
      "F1  0.6887254901960784\n",
      "\there\n",
      "0.9672522669367942\n",
      "0.8145177204920369\n",
      "(array([0, 1]), array([1094,  368]))\n",
      "F1  0.7282463186077643\n",
      "\there\n",
      "0.969982414019467\n",
      "0.8218172924055277\n",
      "(array([0, 1]), array([1070,  405]))\n",
      "F1  0.7225806451612903\n",
      "\there\n",
      "0.970067699435647\n",
      "0.8321027897440624\n",
      "(array([0, 1]), array([994, 467]))\n",
      "F1  0.7482993197278911\n",
      "\there\n",
      "0.9659392603349289\n",
      "0.864602033380139\n",
      "(array([0, 1]), array([978, 481]))\n",
      "F1  0.8030303030303029\n",
      "\there\n",
      "0.9701768118981359\n",
      "0.798352736200206\n",
      "(array([0, 1]), array([1049,  413]))\n",
      "F1  0.7166666666666666\n",
      "0.8232316419106063\n",
      "0.7422499804654838\n"
     ]
    }
   ],
   "source": [
    "folds = 8\n",
    "mean_roc_auc = 0\n",
    "mean_f1 = 0\n",
    "for train_index, test_index in GroupKFold(n_splits=folds).split(XTrain, y_train, groups_train):\n",
    "    \n",
    "    xTrain = -XTrain[train_index]\n",
    "    yTrain = y_train[train_index]\n",
    "    \n",
    "    xTest = -XTrain[test_index]\n",
    "    yTest = y_train[test_index]\n",
    "    clf = RandomForestClassifier(n_estimators=2000, \n",
    "                                 min_samples_leaf=2,\n",
    "                                 min_samples_split=4,\n",
    "                                 max_features=\"log2\",\n",
    "                                 criterion=\"entropy\",\n",
    "                                 max_depth=15,\n",
    "                                 n_jobs=-1,\n",
    "                                 class_weight=\"balanced\"\n",
    "                                    \n",
    "                                )\n",
    "    \n",
    "    clf.fit(xTrain, yTrain)\n",
    "    print(\"\\there\")\n",
    "    res = clf.predict(xTrain)\n",
    "    print(roc_auc_score(yTrain, res))\n",
    "    res = clf.predict(xTest)\n",
    "    print(roc_auc_score(yTest, res))\n",
    "    \n",
    "    a1 = clf.predict(xTest)\n",
    "    threeshold = np.quantile(a1, 1-p)\n",
    "    \n",
    "    mean_roc_auc += roc_auc_score(yTest, a1)\n",
    "    #print(a1, threeshold)\n",
    "    #a1 = (a1 > threeshold).astype(int)\n",
    "    print(np.unique(a1,return_counts=True))\n",
    "    print(\"F1 \", f1_score(yTest, a1))\n",
    "    mean_f1 += f1_score(yTest, a1)\n",
    "\n",
    "\n",
    "print(mean_roc_auc / folds)\n",
    "print(mean_f1 / folds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 4)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_titles = pd.read_csv(\"docs_titles.tsv\", sep=\"\\t\")\n",
    "test_groups = pd.read_csv(\"test_groups.csv\")\n",
    "table = pd.merge(test_groups, all_titles, left_on='doc_id', right_on='doc_id',how=\"left\")\n",
    "table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>group_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11691</td>\n",
       "      <td>130</td>\n",
       "      <td>6710</td>\n",
       "      <td>КАК ПРОПИСАТЬ АДМИНКУ В КС 1.6 СЕБЕ ИЛИ ДРУГУ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11692</td>\n",
       "      <td>130</td>\n",
       "      <td>4030</td>\n",
       "      <td>Скачать: SGL-RP доработка | Слив мода [MySQL] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11693</td>\n",
       "      <td>130</td>\n",
       "      <td>5561</td>\n",
       "      <td>Как прописать админку в кс 1.6 - Counter-Strik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11694</td>\n",
       "      <td>130</td>\n",
       "      <td>4055</td>\n",
       "      <td>Как прописать простую админку в кс 1 6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11695</td>\n",
       "      <td>130</td>\n",
       "      <td>4247</td>\n",
       "      <td>Подбор админов для сервера по КОД_4 [Архив]  -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id  group_id  doc_id  \\\n",
       "0    11691       130    6710   \n",
       "1    11692       130    4030   \n",
       "2    11693       130    5561   \n",
       "3    11694       130    4055   \n",
       "4    11695       130    4247   \n",
       "\n",
       "                                               title  \n",
       "0  КАК ПРОПИСАТЬ АДМИНКУ В КС 1.6 СЕБЕ ИЛИ ДРУГУ ...  \n",
       "1  Скачать: SGL-RP доработка | Слив мода [MySQL] ...  \n",
       "2  Как прописать админку в кс 1.6 - Counter-Strik...  \n",
       "3             Как прописать простую админку в кс 1 6  \n",
       "4  Подбор админов для сервера по КОД_4 [Архив]  -...  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_test = np.array(table[\"group_id\"])\n",
    "texts = np.array(table[\"title\"])\n",
    "for i in range(len(texts)):\n",
    "    if texts[i] is np.nan:\n",
    "        texts[i] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_symbols(text):\n",
    "    res = \"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    for i in text:\n",
    "        t = i    \n",
    "        if not(\n",
    "                (t >= '0' and t <= '9') or \n",
    "                (t >= 'a' and t <= 'z') or\n",
    "                (t >= 'а' and t <= 'я')\n",
    "              ):\n",
    "            t = ' '\n",
    "            \n",
    "        res += t\n",
    "        \n",
    "    return res \n",
    "\n",
    "\n",
    "def list2string(myList):  \n",
    "    myStr = \"\"  \n",
    "    \n",
    "    for word in myList:\n",
    "        if (word not in rus_stopwords) and (word not in eng_stopwords ) and (word not in html_stopwords):\n",
    "            myStr += word\n",
    "    \n",
    "    return myStr\n",
    "\n",
    "def stemming(text):\n",
    "    text = delete_symbols(text)\n",
    "    \n",
    "    tokens = mystem.lemmatize(text)\n",
    "    stri = list2string(tokens)\n",
    "    #print(stri)\n",
    "    return stri\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'привет  медведы   дом   жена\\n'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem = Mystem() \n",
    "rus_stopwords = stopwords.words(\"russian\")\n",
    "eng_stopwords = stopwords.words(\"english\")\n",
    "html_stopwords = [\"span\", \"br\", \"a\", \"href\", \"img\", \"www\", \"com\", \"google\", \"ru\", \"html\", \"http\", \"https\"]\n",
    "stemming(\"Привет, медвед я в доме у  жен\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_texts = []\n",
    "for i in texts:\n",
    "    new_texts.append(stemming(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X_test = vect.fit_transform(new_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_id = []\n",
    "groups_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = 25\n",
    "XTrain = []\n",
    "for i in np.unique(groups_train):\n",
    "    X_ = X_train[groups_train == i]\n",
    "    \n",
    "    distMart = sklearn.metrics.pairwise_distances(X_, metric=\"cosine\")\n",
    "    #print(distMart)\n",
    "    meanDist = np.sort(distMart, axis=1)[:,4:neigh]\n",
    "    lis = []\n",
    "    meanD = np.sort(distMart, axis=1)[:,:].mean(axis=1)\n",
    "    stdD = np.sort(distMart, axis=1)[:,:].std(axis=1)\n",
    "    \n",
    "    \n",
    "    DIST = sklearn.metrics.pairwise.cosine_distances(X_,  X_train[-129 - 1 + i, :])\n",
    "    #print(distMart)\n",
    "    #print(meanD.shape)\n",
    "    for i in range(meanDist.shape[0]):\n",
    "        rr = [j for j in meanDist[i,:]]\n",
    "        rr.append(meanD[i])\n",
    "        rr.append(stdD[i])\n",
    "        #rr.append(DIST[i])\n",
    "        XTrain.append(rr)\n",
    "        #print(i.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#XTrain = np.array(XTrain)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "neigh = 25\n",
    "XTest = []\n",
    "for i in np.unique(groups_test):\n",
    "    X_ = X_test[groups_test == i]\n",
    "    \n",
    "    distMart = sklearn.metrics.pairwise_distances(X_, metric=\"cosine\")\n",
    "    #print(distMart)\n",
    "    meanDist = np.sort(distMart, axis=1)[:,4:neigh]\n",
    "    lis = []\n",
    "    meanD = np.sort(distMart, axis=1)[:,:].mean(axis=1)\n",
    "    stdD = np.sort(distMart, axis=1)[:,:].std(axis=1)\n",
    "    for i in range(meanDist.shape[0]):\n",
    "        rr = [j for j in meanDist[i,:]]\n",
    "        rr.append(meanD[i])\n",
    "        rr.append(stdD[i])\n",
    "        rr.append(maxD[i])\n",
    "        rr.append(minD[i])\n",
    "        XTest.append(rr)\n",
    "        #print(i.shape)\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "#XTrain = np.array(XTrain)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTest = np.array(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16627 (16627, 25)\n"
     ]
    }
   ],
   "source": [
    "with open(\"TEST_DBSCAN_VECTOR.txt\", \"r\") as f:\n",
    "    lis = [int(i) for i in f.read().split()]\n",
    "    print(len(lis),XTest.shape)\n",
    "    XTest = np.append(XTest, np.array(lis).reshape((-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11690, 377), (16627, 377))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain.shape, XTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTest = make_big_matr(XTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[1 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "[0 0 0 ... 1 1 1]\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.000000000001\n",
    "max_epoch = 500\n",
    "C = 0.2\n",
    "batch_size = 450\n",
    "folds = 8\n",
    "res = []\n",
    "threeshold = 0.30\n",
    "mean_roc_auc = 0\n",
    "mean_f1 = 0\n",
    "result = []\n",
    "for train_index, test_index in GroupKFold(n_splits=folds).split(X_train, y_train, groups_train):\n",
    "    #threeshold = 0.1\n",
    "    xTrain = XTrain[train_index]\n",
    "    yTrain = y_train[train_index]\n",
    "    \n",
    "    xTest = XTrain[test_index]\n",
    "    yTest = y_train[test_index]\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=2000, \n",
    "                                 min_samples_leaf=2,\n",
    "                                 min_samples_split=4,\n",
    "                                 max_features=\"log2\",\n",
    "                                 criterion=\"entropy\",\n",
    "                                 max_depth=15,\n",
    "                                 n_jobs=-1,\n",
    "                                 class_weight=\"balanced\"\n",
    "                                    \n",
    "                                )\n",
    "    clf.fit(xTrain, yTrain)\n",
    "    a1 = clf.predict(XTest)\n",
    "    \n",
    "    \n",
    "    threshold = np.quantile(a1, 1-p)\n",
    "    \n",
    "    a1 = (a1 > threeshold).astype(int)\n",
    "    result.append(a1)\n",
    "\n",
    "\n",
    "print(mean_roc_auc / folds)\n",
    "print(mean_f1 / folds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.array(result)\n",
    "r = r.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = 2\n",
    "for i in range(r.shape[0]):\n",
    "    if r[i] > 5:\n",
    "        r[i] = 1\n",
    "    else:\n",
    "        r[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.27671859024478257, 0.28751069289991443)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.array(r)\n",
    "r.mean(), p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"my_submission2.csv\", \"w\") as f:\n",
    "    print(\"pair_id,target\", file=f)\n",
    "    for i in range(len(r)):\n",
    "        print(table[\"pair_id\"].iloc[i], \",\", r[i], file=f, sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.57606601],\n",
       "       [0.57606601, 1.        ]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(XTrain[:,23],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 24)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
